{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development \n",
    "\n",
    "\n",
    "## <span style=\"color:red\">Name: Primary Task</span>   \n",
    "**Primary Task:** One of *Image to Value(s)*, *Image to Image*, or *2D to 3D*  </span>  \n",
    "\n",
    "### <span style=\"color:red\">Name: Primary Focus</span>      \n",
    "<span style=\"color:red\">**Primary Focus:** Describe the main goal of your use case, such as *segmentation* or *object counting*. </span>  \n",
    "\n",
    "### <span style=\"color:red\">Name: Application</span>  \n",
    "<span style=\"color:red\">**Application:** Specify the implemented exemplary application for your use case, e.g., *segmentation of cellular structures*.  </span>  \n",
    "\n",
    "#### <span style=\"color:red\">Name: Challenge</span>    \n",
    "<span style=\"color:red\">**Challenge:** If your use case addresses a specific challenge (e.g., explainability, handling small datasets), mention it here. </span>   \n",
    "\n",
    "#### <span style=\"color:red\">Required Labels</span>  \n",
    "<span style=\"color:red\">**Required Labels:** List the necessary labels for training your model (e.g., *segmentation mask*, *location*).  </span>  \n",
    "\n",
    "---  \n",
    "\n",
    "## TL;DR ðŸ§¬âœ¨\n",
    "<span style=\"color:red\">Add a short description of your use case here.</span>    \n",
    "\n",
    "You can also include a teaser image like this:  \n",
    "`![Teaser](./images/Teaser.png)`  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "This notebook allows you to do: \n",
    "\n",
    "- âœ… **Model Training**: Model training involves optimizing a neural network on a given dataset to learn meaningful patterns, resulting in a trained model that can be used for Evaluation or Inference.  \n",
    "\n",
    "    - âœ… **Hyperparameter Search**: Before full model training, it is recommended to perform a hyperparameter search. This involves multiple short training runs on different hyperparameter sets to approximate full training performance. The best-performing set is then selected. Expanding the search space or using better approximations can improve results but requires more time and compute resources. Make sure you have enough Lightning AI credits before doing so.  \n",
    "\n",
    "    - âœ… **Model Training + Validation**: Once the best hyperparameters are selected, the model is trained on the dataset while monitoring performance on a validation set. This ensures that the model generalizes well and helps prevent overfitting.  \n",
    "\n",
    "- âœ… **Evaluation**: Evaluation assesses the performance of a trained model. This can be done on the test split of the training dataset (this should usually be done after training the model) or on a completely new dataset to check generalization. To evaluate on the full dataset, enable the \"evaluate on full\" checkbox before running the evaluation cell, only do this if you did not train the model on the same data.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports from the template \n",
    "from deepEM.Utils import create_text_widget, print_info, find_file, create_checkbox_widget\n",
    "from deepEM.Logger import Logger\n",
    "from deepEM.ModelTuner import ModelTuner\n",
    "\n",
    "# costum implementation\n",
    "from src.ModelTrainer import ModelTrainer\n",
    "\n",
    "\n",
    "# import all required libraries\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Data Acquisition  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "Provide a brief description of the exemplary data used in your use case.  \n",
    "If necessary, include guidelines or tips for collecting custom datasets.  </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2. Data Anntation\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Include a step-by-step guide on how to annotate data such that it can be used in a plug and play fashion. This should allow EM researchers for example to exchange the segmentation of cellular structures with the segmentation of virus particles to match their labratories needs. \n",
    "\n",
    "For data annotation we recomment using the <a href=\"https://www.cvat.ai/\">CVAT</a> (Computer Vision Annotation Tool) tool. For further instructions, we refer to our <a href=\"\">Getting Started</a> page.\n",
    "\n",
    "Example step-by-step guide is shown below for the case of annotating location annotations. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Create a New Task\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-1.png\" alt=\"Create New Task1\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "When starting CVAT, you first need to create a new task. You can give it a name, add annotation types and upload your data.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-2.png\" alt=\"Create New Task2\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "Next, click on the `Add label` button. Name it based on the class you want to annotate. In our case one of *\"naked\", \"budding\", \"enveloped\"*. As annotation type choose `Points`. You should also pick a color, as this will simplify the annotation process. For adding new class click `Continue`. Once you added all nessecary classes click `Cancel`. \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-3.png\" alt=\"Create New Task3\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "Now you can upload the data you wish to annotate. Finally, click `Submit & Open` to continue with the annotation of the uploaded data. \n",
    "\n",
    "### 1.2.2. Annotation\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-4.png\" alt=\"Annotate1\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "This will open following view. Click on the job (in this view the `Job #1150022`) to start the annotation job. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-5.png\" alt=\"Annotate2\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "To then annotate your data, select the `Draw new points` tool. Select the Label you wish to annotate from the dropdown menue. Then click `Shape` to annotate individual virus capsids with the label class. (Track will allow you to place annotations over multiple frames, which is helpful when annotating videos, tomograms or similar). You can use the arrows on the top middle to navigate through all of your data and to see your annotation progress. \n",
    "\n",
    "### 1.2.3. Save Annotation\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-6.png\" alt=\"Save1\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "Once you are done annotating data, click on the `Menu` and select `Export job dataset`. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-7.png\" alt=\"Save2\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "During export select the `CVAT for Images 1.1` format and give the folder a name. It will prepare the dataset for download. If you have the annotated images stored locally, there is no need to enable `Save Images`. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://viscom-ulm.github.io/DeepEM/static/images/explainable-virus-quantification/CVAT-8.png\" alt=\"Save3\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "In the horizontal menu bar at the top go to `Requests`. It will show a request Export Annotations. On the right of this request click on the three dots on the right to download your exported, annotated data. This will download a .zip file containing the annotation file in .xml format. The name of the file should be \"annotations.xml\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.3. Data Preprocessing\n",
    "\n",
    "<span style=\"color:red\">Include a step-by-step guide for preprocessing the data based on common EM data formats like `.tif`to match your notebooks needs.</span>\n",
    "\n",
    "<span style=\"color:red\">[ImageJ](https://imagej.net/ij/) can be a helpful software in this case, as it is an open source, Java-based image processing software that runs on multiple platforms and offers a wide range of features, including automation with macros, extensive community support, and a large library of tools and plugins.</span>\n",
    "\n",
    "<span style=\"color:red\">In the following we showcase an example usecase to import and export data into required formats. </span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/ImageJ-1.png\" alt=\"ImageJ1\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "<span style=\"color:green\">This tool allows to `import` a large amount of different, commonly used file formats im EM. </span>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/ImageJ-2.png\" alt=\"ImageJ2\" width=\"500\">\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "<span style=\"color:green\">Using the provided `Save As..` functionality allows to save the imported files as a `Image Sequence` in .tif format or single `.tif` files.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Data Structuring\n",
    "\n",
    "<span style=\"color:red\">Your use case will require the data to be in a specific structure, such that it can be used for  training. Describe this structure here. </span>\n",
    "\n",
    "<span style=\"color:green\">**An example description could look like this:** </span>\n",
    "\n",
    "<span style=\"color:green\">The provided notebook requires that all training, validation and testing data is placed within a single folder. Splitting the data into train, test and validation will be done during runtime. </span>\n",
    "\n",
    "<span style=\"color:green\">Additionally, the generated `annotations.xml` should be put in the same folder as the .tif images.</span>\n",
    "\n",
    "<span style=\"color:green\">You can check the exemplary data provided at `data/tem-herpes/` for clarification.</span>\n",
    "\n",
    "<span style=\"color:green\">An example with five images and the corresponding annotation is shown below: </span>\n",
    "\n",
    "```\n",
    "/data/tem-herpes/\n",
    "â”œâ”€â”€ image_001.tif\n",
    "â”œâ”€â”€ image_002.tif\n",
    "â”œâ”€â”€ image_003.tif\n",
    "â”œâ”€â”€ image_004.tif\n",
    "â”œâ”€â”€ image_005.tif\n",
    "â””â”€â”€ annotations.xml\n",
    "\n",
    "```\n",
    "\n",
    "> *Execute the cell below to show a text form. Within this text form you need to define the path to your training data (i.e. `data/`).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb37a6f30bd434f936990ebde9278e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='./data', description='Data Path:', layout=Layout(width='1000px'), style=TextStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c3f601299d40d085bdd1967cc96906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Enter the path to your data folder.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_widget = create_text_widget(\"Data Path:\",\"./data\",\"Enter the path to your data folder.\")\n",
    "display(*data_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set and check the provided Data Path from the text form above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Data path was set to: ./data\n"
     ]
    }
   ],
   "source": [
    "data_path = data_widget[0].value\n",
    "print(f\"[INFO]::Data path was set to: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Setup Logging\n",
    "\n",
    "By executing the cell below, we setup the logging directory for the hyperparameter search, model training and evaluation. \n",
    "The logger creates a folder at `./logs/<datafoldername>-<currentdatetime>/`. \n",
    "\n",
    "For each training run there will be one subfolder within the log directory. Training runs of hyperparameter sweeps are called `Sweep_<idx>`, while the subfolder of the final training run is called `TrainingRun`. During evaluation there will be one more subfolder created called `Evaluate`. \n",
    "\n",
    "Within each subfolder folder there will be logging of: \n",
    "\n",
    "- the used hyperparameters, (`<log-path>/<subfolder>/hyperparameters.json`)\n",
    "- the best performing model checkpoint based on the validation loss (`<log-path>/<subfolder>/checkpoints/best_model.pth`)\n",
    "- the last model checkpoint (`<log-path>/<subfolder>/checkpoints/latest_model.pth`)\n",
    "- visualizations of training and validation curves (`<log-path>/<subfolder>/plots/training_curves.png`)\n",
    "- qualitative visualization of sampled validation images (`<log-path>/<subfolder>/samples/`)\n",
    "- results on test metrics (`<log-path>/<subfolder>/test_results.txt`)\n",
    "- qualitative visualization of sampled test images (`<log-path>/<subfolder>/samples/`)\n",
    "\n",
    "\n",
    "<span style=\"color:red\">Add a short description of what the sample visualizations show in your use case.</span>\n",
    "\n",
    "Visualization of training and validation curves will also be shown after every successful training run within this notebook. They can help to identify possible issues like overfitting during training. For more details, we refer to [this guide](https://www.kaggle.com/code/ryanholbrook/overfitting-and-underfitting).\n",
    "\n",
    "> *Exectue the cell below to setup the logger. **Hint** If you wish to train a new model, you can reexecute this cell, to generate a new log directory - allowing you to now override the previousely trained model.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 09:12:08,726 - INFO - Logger initialized. Logs will be saved to: logs/data_2025-03-31_09-12-08\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters in deep learning control how a model is trained. Unlike learned model parameters, they are set before training. Hyperparameter tuning explores different configurations by training the model multiple times and selecting the best-performing settings based on validation performance. Since this process is time- and resource-intensive, training runs are often limited in duration or dataset size.\n",
    "\n",
    "Our playground automates hyperparameter search using grid search, testing all possible combinations of selected hyperparameters. The search space is initially defined by deep learning (DL) experts, who also provide explanations so electron microscopy (EM) specialists can refine it as needed. In order to do so, you can adapt the form below. Each sweep hyperparameter should be separated by `,`. Floating point values should be written like `0.1`. \n",
    "Logging estimates the remaining time for individual runs and the full sweep, though early estimates may be inaccurate.\n",
    "\n",
    "While not strictly required, a hyperparameter search should be performed at least once per dataset to ensure optimal model performance. Interrupting the search early may yield suboptimal results. The automatic sweep stores tested configurations and the best-performing parameters in the training data directory under `Sweep_Parameters`, allowing reuse for future training, or to resume the sweep if the kernel got interrupted.\n",
    "\n",
    "\n",
    "> *Execute the cell below to show the form of the hyperparameter search space. **Hint** If you've changed parameters and want to reset them to the defaults, reexecute the cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b64efbabf0428baad95d145dc3d954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Hyperparameter Sweep</h1>'), HTML(value='<p>During hyperparameter sweeps it canâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hyperparameter search\n",
    "model_trainer = ModelTrainer(data_path, logger)\n",
    "\n",
    "hyperparameter_tuner = ModelTuner(model_trainer, data_path, logger)\n",
    "form = hyperparameter_tuner.create_hyperparameter_widgets()\n",
    "display(form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *If you wish to run a hyperparameter sweep based on the parameters above, please execute the cell below. This should be done at least once per dataset. Note that this can take a while.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 09:14:20,893 - INFO - Start hyperparameter sweep...\n",
      "2025-03-31 09:14:20,895 - INFO - Logger initialized. Logs will be saved to: logs/data_2025-03-31_09-12-08/Sweep_0\n",
      "2025-03-31 09:14:20,895 - INFO - Start Sweep 1 of 6...\n",
      "2025-03-31 09:14:20,896 - INFO - Current hyperparams {'learning_rate': 0.0001, 'batch_size': 4}\n",
      "2025-03-31 09:14:20,896 - INFO - Could not find a sweep configuration at ./data/Sweep_Parameters/best_sweep_parameters.json.\n",
      "2025-03-31 09:14:20,898 - INFO - Hyperparameters saved to logs/data_2025-03-31_09-12-08/Sweep_0/hyperparameters.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset: 0.5\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Run] | Num Epochs: 10 | Dataset size: 25000:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "2025-03-31 09:14:22,262 - ERROR - An error occurred during hyperparameter search with following parameters: \n",
      "{'learning_rate': 0.0001, 'batch_size': 4}\n",
      "2025-03-31 09:14:22,262 - ERROR - Error: \n",
      "ModelTrainer.train_step() takes 2 positional arguments but 3 were given\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'val_loss' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m best_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m hyperparameter_tuner\u001b[38;5;241m.\u001b[39mupdate_config(form)\n\u001b[0;32m----> 3\u001b[0m best_config \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/hansel/SSD/Code/PaulFestschrift/Deep-EM Playground/DeepEM-Template/deepEM/ModelTuner.py:151\u001b[0m, in \u001b[0;36mModelTuner.tune\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"method for tuning to be implemented by DL specialists.\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 151\u001b[0m     best_params, best_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished sweep with best validation loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWill use these hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/hansel/SSD/Code/PaulFestschrift/Deep-EM Playground/DeepEM-Template/deepEM/ModelTuner.py:135\u001b[0m, in \u001b[0;36mModelTuner.tune_grid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_error(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred during hyperparameter search with following parameters: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mhyperparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_error(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_sweepparameters(hyperparams, \u001b[43mval_loss\u001b[49m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(updated):\n\u001b[1;32m    137\u001b[0m     best_index \u001b[38;5;241m=\u001b[39m index                \n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'val_loss' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "best_config = None\n",
    "hyperparameter_tuner.update_config(form)\n",
    "best_config = hyperparameter_tuner.tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our automatic hyperparameter tuning is able to find the best performing set of hyperparameters based on the setting shown above. \n",
    "\n",
    "However, there can be scenarios, where additional flexibility is required. Therefore, you are able to change these hyperparameters in the following. \n",
    "\n",
    "> *Execute the cell below to show and possibly adapt the currently chosen hyperparameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a77df02d704485cb1168b41558a297e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Could not find best hyperparameters for current dataset (data). Make sure to conâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "form = hyperparameter_tuner.edit_hyperparameters()\n",
    "display(form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can increase the number of training epochs. An 'epoch' in deep learning is one full pass of the model through all the training data, where the model learns and adjusts to improve its predictions. Higher number of epochs leads to longer training but can further improve model performance.\n",
    "\n",
    "> *Execute the cell below to show and possibly adapt the number of training epochs. Leave as is, if you want to train with the suggestion of the DL expert.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ddc80f4fdc49a19439b631e3fed379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='20', description='Epochs:', layout=Layout(width='1000px'), style=TextStyle(description_width='initâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb679ba60664b61ba66a8adde69fb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Higher number of epochs leads to longer training but can further improve model perforâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_trainer.reduce_epochs = None\n",
    "model_trainer.set_epochs()\n",
    "epochs_widget = create_text_widget(\"Epochs:\",str(model_trainer.num_epochs),\"Higher number of epochs leads to longer training but can further improve model performance.\")\n",
    "display(*epochs_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below to set the hyperparameters and number of training epochs for your training run, based on the forms above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]::Will use following hyperparameters for future training: {'learning_rate': 0.001, 'batch_size': 4} with number of epochs: 20.\n"
     ]
    }
   ],
   "source": [
    "best_config = hyperparameter_tuner.update_hyperparameters(form)\n",
    "model_trainer.num_epochs = int(epochs_widget[0].value)\n",
    "print_info(f\"Will use following hyperparameters for future training: {best_config} with number of epochs: {model_trainer.num_epochs}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training and Validation\n",
    "\n",
    "In this section we train and validate the model based on the provided data and hyperparameters resulting from the previous sweep.\n",
    "\n",
    "Training in deep learning is the process where a model learns patterns from labeled data (the one provided at the top of this notebook) by optimizing its parameters through backpropagation. \n",
    "Validation involves using a separate dataset to evaluate the model's performance during training, ensuring it generalizes well to unseen data.\n",
    "\n",
    "Training and validating a model can take a lot of time (ranging from minutes to hours, days or even weeks) depending on the model, the training procedure and the dataset. Our logging module provides approximate times for training, which you can see below the executed training cell or at the `log.txt` within the current log directory (i.e. `<log-dir>/TrainingRun/`). However, these times can be inaccurate, especially at the beginning of training. \n",
    "\n",
    "### Model Checkpoint\n",
    "In the following you can provide a model checkpoint for training. There are two different scenarios when you might want to provide a model checkpoint:\n",
    "\n",
    "1. You wish to resume training. This means, training will picks up exactly where it left off, including learned patterns and settings. This is useful if training was interrupted and needs to be finished from the last saved state. To do so, you need to provide a model checkpoint in the text form below. You can find the last saved checkpoint inside the runs logging directory (`<log-dir>/TrainingRun/checkpoints/latest_model.pth`).\n",
    "2. If you wish to finetune your model. Fine-tuning starts training from the beginning (epoch 0) but uses a pre-trained model as a starting point, already having knowledge about some previousely learned patterns, to improve its performance on a new task or dataset. You can find the best model checkpoint inside the runs logging directory (`<log-dir>/TrainingRun/checkpoints/best_model.pth`).\n",
    "\n",
    "If you wish to finetune the model, you need to check the checkbox below. If you only provide the path to a directory, it will look for a `best_model.pth` or `latest_model.pth` accordingly, within this directory.\n",
    "\n",
    "\n",
    "If you want to start training from scratch (which is usually the case), you can leave the text form below empty.\n",
    "\n",
    "> *Execute the cell below to show a text form. If you wish to resume training, or do finetuning you need to provide a path to a model checkpoint. Leave it empty for standard training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b035011ce8084e3183d8d0e1dee87523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Checkpoint Path:', layout=Layout(width='1000px'), style=TextStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260e706951c14545b70d3294ac955cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> If you wish to resume an earlier training, or do finetuning, enter the path to the laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1566ff00b0f47ac87943f8b957b359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Enable Finetuning', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b964cf576d244c20806bc8f74acfb278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> Check this box to do finetuning based on the provided model checkpoint above. This meâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resume_widget = create_text_widget(\"Model Checkpoint Path:\",\"\",\"If you wish to resume an earlier training, or do finetuning, enter the path to the latest_model.pth or the best_model.pth file here.\")\n",
    "checkbox_widget, description_widget = create_checkbox_widget(\n",
    "    name=\"Enable Finetuning\",\n",
    "    value=False,\n",
    "    description=\"Check this box to do finetuning based on the provided model checkpoint above. This means, the models weights will only be used for initializing the model, training will be then done as usual.\"\n",
    ")\n",
    "display(*resume_widget, checkbox_widget, description_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 09:17:24,184 - INFO - Logger initialized. Logs will be saved to: logs/data_2025-03-31_09-12-08/TrainingRun\n",
      "2025-03-31 09:17:24,185 - INFO - Hyperparameters saved to logs/data_2025-03-31_09-12-08/TrainingRun/hyperparameters.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "resume_training = resume_widget[0].value\n",
    "finetuning = checkbox_widget.value\n",
    "if(resume_training):\n",
    "    if(os.path.isfile(resume_training)):\n",
    "        logger.log_info(f\"Found model checkpoint at {resume_training}.\")\n",
    "\n",
    "    else: \n",
    "        if(not finetuning):\n",
    "            resume_training = find_file(resume_training, \"latest_model.pth\")           \n",
    "        else: \n",
    "            resume_training = find_file(resume_training, \"best_model.pth\")\n",
    "            \n",
    "            \n",
    "        if(resume_training is None):\n",
    "            logger.log_error(f\"Could not find resume path at {resume_widget[0].value}. Will start training from scatch.\")\n",
    "        else: \n",
    "            logger.log_info(f\"Found model checkpoint at {resume_training}.\")\n",
    "\n",
    "else:\n",
    "    resume_training = None\n",
    "logger.init(\"TrainingRun\")\n",
    "model_trainer.resume_from_checkpoint = resume_training\n",
    "model_trainer.finetuning = finetuning\n",
    "model_trainer.prepare(best_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Execute the cell below if you wish to **train** a model. Note that this can take a while.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Run] | Num Epochs: 20 | Dataset size: 50000:   0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ModelTrainer.train_step() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/hansel/SSD/Code/PaulFestschrift/Deep-EM Playground/DeepEM-Template/deepEM/ModelTrainer.py:566\u001b[0m, in \u001b[0;36mAbstractModelTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs), initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Training Run] | Num Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    565\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 566\u001b[0m     train_loss, val_loss, early_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    568\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/media/hansel/SSD/Code/PaulFestschrift/Deep-EM Playground/DeepEM-Template/deepEM/ModelTrainer.py:449\u001b[0m, in \u001b[0;36mAbstractModelTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[0;32m--> 449\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# Step the learning rate scheduler if available\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ModelTrainer.train_step() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "model_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation\n",
    "Evaluation in deep learning is the process of evaluating a trained model on a separate, unseen dataset to measure its final performance. It provides an unbiased assessment of the model's ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Choose Model \n",
    "\n",
    "In this section we choose the model for testing. \n",
    "If you leave the `Model Path` empty in the text form below, it will use the last model trained.\n",
    "Otherwise, you can define the path to the models best weights at `<log-path>/TrainingRun/checkpoints/best_model.pth` or by providing a path to a directory, which contains `best_model.pth` (like `<log-path>/TrainingRun/`). This allows you to also test shared models or previousely trained models.\n",
    "\n",
    "> *Execute the cell below to show the text form for selecting a model for testing. Leave empty to test the previouely trained model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9101d57044494144ba2744ab602409f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Model Path:', layout=Layout(width='1000px'), style=TextStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe9b3ee87944ce480da97a3910d9a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Hint:</b> If you wish to test a specific model, you can here define the path to its checkpoint â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"\",\"If you wish to test a specific model, you can here define the path to its checkpoint (for example: logs/tem-herpes_2025-02-03_11-42-43/TrainingRun/checkpoints). Leave empty to test the last trained model.\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Evaluate\n",
    "We finally evaluate the provided model on the test set. We investigate following metrics: \n",
    "\n",
    "<span style=\"color:red\"> Add a description of the used metrics here. </span>\n",
    "\n",
    "<span style=\"color:green\">\n",
    "\n",
    "- **Mean Squared Error (MSE)**: A distance metric to compute the squared difference between the prediction and the label. In this use case this means, we compute the difference between the real EM tilt images with the computed EM images based on the learned reconstruction. If MSE is low, the model is able to correctly represent the underlying sample within the tomogram. The lower the metric, the better. \n",
    "\n",
    "- **MSE phantom**: The first metric is not able to capture generalizability of the model to unknown tilt angles, as it only computes the MSE on the tilt series which was used for training. But as we are using synthetic data, we do have access to the underlying phantom volume (the synthetic sample used to compute the tilt series). We hence compare the learned sample with this phantom volume, again by computing the MSE. \n",
    "\n",
    "#### **Summary**\n",
    "| Metric  | Meaning | Interpretation |\n",
    "|---------|---------|---------|\n",
    "| **MSE**  | Distance between true tilt series and tilt series based on the learned reconstruction. | Lower is better |\n",
    "| **MSE phantom**  | Distance between the phantom volume (the synthetic sample used to compute the tilt series) and the learned sample (tomogram) | Lower is better | \n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "<span style=\"color:red\">Add a description of your visualizations here.</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visualizations are saved to `<log-path>/Evaluate/samples/test_*`. \n",
    "\n",
    "> *If you wish to evaluate a model (recommended), execute the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "start_evaluation = False\n",
    "eval_model = model_widget[0].value\n",
    "if(eval_model):\n",
    "    eval_model = Path(eval_model)\n",
    "    if(eval_model.is_dir()):\n",
    "        eval_model = Path(find_file(eval_model, \"best_model.pth\")) \n",
    "    if(not eval_model.is_file()):\n",
    "        logger.log_error(f\"Could not find model at {eval_model}. Make sure to train a model before evaluation.\")\n",
    "        eval_model = None\n",
    "    else: \n",
    "        start_evaluation = True\n",
    "else:\n",
    "    recent_logs = logger.get_most_recent_logs()\n",
    "    eval_model = \"\"\n",
    "    for dataname, log_path in recent_logs.items():\n",
    "        if(dataname == Path(data_path).stem):\n",
    "            eval_model = Path(log_path+\"/TrainingRun/checkpoints/best_model.pth\")\n",
    "            if(not eval_model.is_file()):\n",
    "                logger.log_error(f\"Could not find a trained model at {eval_model}. Make sure you fully train a model first before evaluating.\")\n",
    "            else:\n",
    "                logger.log_info(f\"Found most recent log at {eval_model}\")\n",
    "                start_evaluation = True\n",
    "        else: \n",
    "            continue\n",
    "    if(not start_evaluation):\n",
    "        logger.log_error(\"Could not find a trained model. Make sure you train a model first before evaluating.\")\n",
    "      \n",
    "if(start_evaluation):\n",
    "    evaluate_on_full = False\n",
    "    model_trainer.load_checkpoint(eval_model)\n",
    "    model_trainer.test(evaluate_on_full)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whats Next?\n",
    "\n",
    "## Not satisfied? \n",
    "If you are not satisfied with the evaluation performance of your model you have multiple options: \n",
    "1. Extend the hyperparameter search by adding more, different hyperparameters - they might work better.\n",
    "2. Extend the hyperparameter search to a larger data subset or longer training. This will lead to more accurate results of the sweep, but it will require more compute time. \n",
    "3. Run multiple training runs with different custom parameters (you can define them right before the cell for training your model).\n",
    "4. Extend the number of training epochs. Longer training can lead to better performance of the model. To do so, you can again define the number of epochs in the cell right before training the model.\n",
    "5. Finetune or train the model on your own annotated dataset. To do so, we recomment using CVAT for annotation. A general guide for annotating your data with CVAT can ge found on our [\"Getting Started - 4. Data Annotation\"](https://viscom-ulm.github.io/DeepEM/getting-started.html). A more specific guide for this use case can be found at the top of this notebook. \n",
    "6. Check the training and sweep runs loggings and train/val curves - maybe you found an issue with the training itself?\n",
    " \n",
    "## Satisfied?\n",
    "If you are satisfied with the results of your trained or evaluated model, there are multiple things to test next:\n",
    "\n",
    "1. Check the generalizability of your trained model. To do so, you can evaluate your model on different, annotated datasets. Upload your annotated data and define it as `data_path` at the top of this notebook. Check the top of the notebook for data structuring and annotation formats. Run all cells except for `hyperparameter tuning`  and `training`. Then, check the checkbox \"evaluate on full dataset\" such that all of your uploaded data is considered for evalutation. \n",
    "2. Use the trained model for inference. That means using the model on unseen, unlabeled data for the support of your EM data analysis. To do so, open the `2_Inference.ipynb` and follow the steps provided.\n",
    "3. Share and collaborate with other researchers\n",
    "\n",
    "Additionally, you can share your training code and model weights with other collegues. An easy way on how to do this can be found on our website under [\"Getting Started - 5. Collaboration\"](https://viscom-ulm.github.io/DeepEM/getting-started.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepEM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
