{
    "parameter": {
        "epochs": {
            "value": 20,
            "explanation": "An 'epoch' in deep learning is one full pass of the model through all the training data, where the model learns and adjusts to improve its predictions."
        }, 
        "early_stopping_patience": {
            "value": 5,
            "explanation": "'Early stopping patience' is a setting that tells the model to stop training if it doesn't improve after a certain number of tries (validation steps), helping to avoid wasting time on unnecessary training."
        },
        "validation_interval": {
            "value": 1,
            "explanation": "The 'validation interval' is the frequency (in epochs) at which the model checks its performance on a separate set of data during training to see how well it's learning."
        },
        "scheduler_step_by": {
            "value": "epoch", 
            "explanation": "The 'scheduler step by' setting determines whether the optimizer adjusts its learning rate after each 'epoch' (a full pass through the data) or after each 'iteration' (a single step of training on a batch of data)."
        }, 
        "images_to_visualize": {
            "value": 4,
            "explanation": "'Images to visualize' is the number of sample images saved during validation or testing to help evaluate how well the model is making predictions."
        }
    },

    "train_subset": 0.5, 
    "reduce_epochs": 0.5, 
    "method": "grid", 
    "hyperparameter": {
        "learning_rate": {
            "default": 0.001,
            "values": [0.0001, 0.001, 0.01],
            "explanation": "Controls the step size at each iteration while optimizing the model's weights. A higher learning rate may lead to faster convergence but risks overshooting the optimal solution, while a lower rate ensures smoother convergence but may require more training time."
        },
        "batch_size": {
            "default": 4,
            "values": [4,8],
            "explanation": "The number of samples per batch during training. Higher numbers usually lead to more stable training, but eventually lead to 'OOM (out of memory)' errors."
        }
    }
    
    
}