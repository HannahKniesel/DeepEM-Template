{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference \n",
    "Inference in deep learning is the process of using a trained model to make predictions on new, unseen data. It involves applying the learned parameters without further updates to generate outputs such as classifications or predictions.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from IPython.display import display\n",
    "\n",
    "from deepEM.Utils import create_text_widget\n",
    "\n",
    "from src.Inferencer import Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference can be done on any <span style=\"color:green\">2D .tif files</span>. \n",
    "Please define the path to the <span style=\"color:green\">.tif</span> file in `Data Path`. \n",
    "Here you can either define a directory, then inference will be done on all <span style=\"color:green\">.tif files</span> inside this directory, or you define the path to a single <span style=\"color:green\">.tif file</span>. \n",
    "\n",
    "For example usage you can define `./data/inference/`\n",
    "\n",
    "``` \n",
    "\n",
    "./data/inference/\n",
    "├── image_001.tif\n",
    "├── image_002.tif\n",
    "└── image_003.tif\n",
    "\n",
    "```\n",
    "\n",
    "Or you define `./data/inference/#17_1704_1a_2.7um_A_AC_25k_3.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_widget = create_text_widget(\"Data Path:\",\"./data/data/inference\", \"Enter the path to a directory which contains .tif files you wish to do inference on.\")\n",
    "batch_widget = create_text_widget(\"Batch Size:\", 16, \"Please set the batch size for inference. Larger batch size can lead to faster computation but may lead to OOM (out of memory) errors.\")\n",
    "\n",
    "\n",
    "display(*data_widget)\n",
    "display(*batch_widget)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_widget[0].value\n",
    "batch_size = int(batch_widget[0].value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_widget = create_text_widget(\"Model Path:\",\"\", \"Enter the path to a pretrained model which you'd like to use for inference.\")\n",
    "display(*model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Make Prediction\n",
    "By executing the cell below, your trained model will be used to make predictions on the defined data. The predictions will be saved inside the folder of your data at `<currentdatetime>`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = model_widget[0].value\n",
    "inferencer = Inference(model_path, data_path, batch_size)\n",
    "inferencer.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-to-value",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
